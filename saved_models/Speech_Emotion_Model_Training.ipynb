{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGfVek11JTRS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display\n",
        "from IPython.display import Audio\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)  # forcibly remounting for clarity"
      ],
      "metadata": {
        "id": "oLqeAm_1Jiov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/drive/MyDrive/Datasets/TESS'\n",
        "paths = []\n",
        "labels = []\n",
        "for dirname, _, filenames in os.walk(dataset_path):\n",
        "    for filename in filenames:\n",
        "        paths.append(os.path.join(dirname, filename))\n",
        "        label = filename.split('_')[-1]\n",
        "        label = label.split('.')[0]\n",
        "        labels.append(label.lower())\n",
        "    if len(paths) == 2800:\n",
        "        break\n",
        "print('Dataset is Loaded')"
      ],
      "metadata": {
        "id": "I-y4q2ujJkj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create a dataframe\n",
        "df = pd.DataFrame()\n",
        "df['speech'] = paths\n",
        "df['label'] = labels\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ufgGN9F8JkT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['label']"
      ],
      "metadata": {
        "id": "mlAuPPuN4nDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'].value_counts()"
      ],
      "metadata": {
        "id": "gRbfiALyJkNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(data=df, x='label')"
      ],
      "metadata": {
        "id": "uIGAYBVRJkFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def waveplot(data, sr, emotion):\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.title(emotion, size=20)\n",
        "    librosa.display.waveshow(data, sr=sr)\n",
        "    plt.show()\n",
        "\n",
        "def spectogram(data, sr, emotion):\n",
        "    x = librosa.stft(data)\n",
        "    xdb = librosa.amplitude_to_db(abs(x))\n",
        "    plt.figure(figsize=(11,4))\n",
        "    plt.title(emotion, size=20)\n",
        "    librosa.display.specshow(xdb, sr=sr, x_axis='time', y_axis='hz')\n",
        "    plt.colorbar()"
      ],
      "metadata": {
        "id": "tQRFOWgYJ5Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion = 'fear'\n",
        "path = np.array(df['speech'][df['label']==emotion])[0]\n",
        "data, sampling_rate = librosa.load(path)\n",
        "waveplot(data, sampling_rate, emotion)\n",
        "spectogram(data, sampling_rate, emotion)\n",
        "Audio(path)"
      ],
      "metadata": {
        "id": "iqLEsvclJ40V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_mfcc(filename):\n",
        "    y, sr = librosa.load(filename, duration=3, offset=0.5)\n",
        "    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n",
        "    return mfcc\n",
        "\n",
        "def extract_mel(filename):\n",
        "    y, sr = librosa.load(filename, duration=3, offset=0.5)\n",
        "    mel = np.mean(librosa.feature.melspectrogram(y=y, sr=sr).T, axis=0)\n",
        "    return mel\n",
        "\n",
        "def extract_chroma(filename):\n",
        "    y, sr = librosa.load(filename, duration=3, offset=0.5)\n",
        "    chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)\n",
        "    return chroma"
      ],
      "metadata": {
        "id": "ux95359cJ4px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(filename):\n",
        "    y, sr = librosa.load(filename, duration=3, offset=0.5)\n",
        "    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n",
        "    mel = np.mean(librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128).T, axis=0)\n",
        "    chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=12).T, axis=0)\n",
        "    return np.concatenate((mfcc, mel, chroma))\n",
        "\n"
      ],
      "metadata": {
        "id": "BzIrHn5qVj3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_mfcc(df['speech'][0])"
      ],
      "metadata": {
        "id": "g1Y8DEsHKKDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_mel(df['speech'][0])"
      ],
      "metadata": {
        "id": "YI6bCKNSVGDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_chroma(df['speech'][0])"
      ],
      "metadata": {
        "id": "P5kPM-CTVF4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_mfcc = df['speech'].apply(lambda x: extract_mfcc(x))\n",
        "X_features = df['speech'].apply(lambda x: extract_features(x))"
      ],
      "metadata": {
        "id": "vLeZo-sSKJ7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_mfcc\n",
        "X_features"
      ],
      "metadata": {
        "id": "KHYHKqffKJzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X = [x for x in X_mfcc]\n",
        "# X = np.array(X)\n",
        "# X.shape\n",
        "X = [x for x in X_features]\n",
        "X = np.array(X)\n",
        "X.shape"
      ],
      "metadata": {
        "id": "trM5yq1tKJs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## input split\n",
        "X = np.expand_dims(X, -1)\n",
        "X.shape"
      ],
      "metadata": {
        "id": "d0OLddhNJj9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "# Convert integer encoded labels to one-hot encoded labels\n",
        "y = to_categorical(y_encoded)"
      ],
      "metadata": {
        "id": "y3nsXU9oKg9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "id": "OoBth1rfKgqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the encoder object\n",
        "import pickle\n",
        "\n",
        "encoder_path = '/content/drive/MyDrive/Datasets/new_label_encoder.pkl'  # Define the path to save the encoder file\n",
        "with open(encoder_path, 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)"
      ],
      "metadata": {
        "id": "WP-QJBl4LBib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create a dictionary to map encoded labels to emotions\n",
        "# encoded_emotions = dict(zip(range(len(label_encoder.classes_)), label_encoder.classes_))\n",
        "\n",
        "# # Print the encoded labels and their corresponding emotions\n",
        "# for encoded_label, emotion in encoded_emotions.items():\n",
        "#     print(f\"{encoded_label}: {emotion}\")\n"
      ],
      "metadata": {
        "id": "7X1Pg8t-XASH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "best_accuracy = 0\n",
        "best_epochs = 0\n",
        "best_batch_size = 0\n",
        "\n",
        "# Define ranges for epochs and batch size\n",
        "epoch_range = range(50, 101, 10)\n",
        "batch_size_range = range(16, 41, 4)\n",
        "\n",
        "for epochs in epoch_range:\n",
        "    for batch_size in batch_size_range:\n",
        "        print(f\"Training with epochs={epochs} and batch_size={batch_size}...\")\n",
        "\n",
        "        # Define the model\n",
        "        model = Sequential([\n",
        "            LSTM(256, return_sequences=False, input_shape=(X.shape[1], X.shape[2])),\n",
        "            Dropout(0.2),\n",
        "            Dense(128, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(7, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "        # Create a checkpoint to save the best model based on validation accuracy\n",
        "        checkpoint = ModelCheckpoint('/content/drive/MyDrive/Datasets/new_best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "        # Train the model with checkpoint callback\n",
        "        history = model.fit(X, y, validation_split=0.2, epochs=epochs, batch_size=batch_size, callbacks=[checkpoint], verbose=0)\n",
        "\n",
        "        # Evaluate model\n",
        "        val_accuracy = max(history.history['val_accuracy'])\n",
        "        print(f\"Validation accuracy: {val_accuracy}\")\n",
        "\n",
        "        # Check if this is the best accuracy so far\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            best_epochs = epochs\n",
        "            best_batch_size = batch_size\n",
        "\n",
        "print(f\"Best validation accuracy: {best_accuracy}\")\n",
        "print(f\"Best epochs: {best_epochs}\")\n",
        "print(f\"Best batch size: {best_batch_size}\")\n"
      ],
      "metadata": {
        "id": "WLPzMtyVqG28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, LSTM, Dropout\n",
        "# from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# model = Sequential([\n",
        "#     LSTM(256, return_sequences=False, input_shape=(X.shape[1], X.shape[2])),   #(40,1)\n",
        "#     Dropout(0.2),\n",
        "#     Dense(128, activation='relu'),\n",
        "#     Dropout(0.2),\n",
        "#     Dense(64, activation='relu'),\n",
        "#     Dropout(0.2),\n",
        "#     Dense(7, activation='softmax')\n",
        "# ])\n",
        "\n",
        "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# model.summary()\n",
        "\n",
        "# # Create a checkpoint to save the best model based on validation accuracy\n",
        "# checkpoint = ModelCheckpoint('/content/drive/MyDrive/Datasets/newbest_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "# # Train the model with checkpoint callback\n",
        "# history = model.fit(X, y, validation_split=0.2, epochs=50, batch_size=40, callbacks=[checkpoint])\n"
      ],
      "metadata": {
        "id": "PKlNd36HKgjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Print information about the saved model\n",
        "# print(\"Best model training completed.\")\n",
        "# print(\"Accuracy:\", history.history['accuracy'])\n",
        "# print(\"Validation Accuracy:\", history.history['val_accuracy'])\n",
        "# print(\"Loss:\", history.history['loss'])\n",
        "# print(\"Validation Loss:\", history.history['val_loss'])\n",
        "# print(\"Label encoder saved as label_encoder.pkl.\")\n"
      ],
      "metadata": {
        "id": "ScvbFZ5xJjxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "from keras.models import load_model\n",
        "import pickle\n",
        "\n",
        "# Load the saved model\n",
        "saved_model_path = '/content/drive/MyDrive/Datasets/new_best_model.h5'\n",
        "loaded_model = load_model(saved_model_path)\n",
        "\n",
        "# Load the encoder\n",
        "encoder_path = '/content/drive/MyDrive/Datasets/new_label_encoder.pkl'\n",
        "with open(encoder_path, 'rb') as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "\n",
        "# Create a dictionary to map encoded labels to emotions\n",
        "encoded_emotion = dict(zip(range(len(label_encoder.classes_)), label_encoder.classes_))\n",
        "\n",
        "# Print the encoded labels and their corresponding emotions\n",
        "for encoded_label, emotion in encoded_emotion.items():\n",
        "    print(f\"{encoded_label}: {emotion}\")\n",
        "\n",
        "# Define a function to extract MFCC, Mel, and Chroma features from an audio file\n",
        "def extract_features(filename):\n",
        "    y, sr = librosa.load(filename, duration=3, offset=0.5)\n",
        "    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n",
        "    mel = np.mean(librosa.feature.melspectrogram(y=y, sr=sr).T, axis=0)\n",
        "    chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)\n",
        "    return np.concatenate((mfcc, mel, chroma))\n",
        "\n",
        "# Define a function to predict emotion from an audio file\n",
        "def predict_emotion(audio_file):\n",
        "    # Extract features from the audio file\n",
        "    features = extract_features(audio_file)\n",
        "    # Reshape the features for model input\n",
        "    features = np.expand_dims(features, axis=0)\n",
        "    features = np.expand_dims(features, axis=-1)\n",
        "    # Predict the emotion using the loaded model\n",
        "    predicted_probabilities = loaded_model.predict(features)\n",
        "    # Get the predicted emotion label index\n",
        "    predicted_emotion_index = np.argmax(predicted_probabilities)\n",
        "    # Decode the predicted emotion label\n",
        "    predicted_emotion_label = label_encoder.classes_[predicted_emotion_index]\n",
        "    return predicted_emotion_label\n"
      ],
      "metadata": {
        "id": "uQBOSwZkiKeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Define the path to the audio file you want to test\n",
        "# audio_file_path = '/content/drive/MyDrive/Datasets/TESS Toronto emotional speech set data/OAF_happy/OAF_wife_happy.wav'\n",
        "\n",
        "# # Predict the emotion from the audio file\n",
        "# predicted_emotion = predict_emotion(audio_file_path)\n",
        "\n",
        "# print(\"Predicted Emotion:\", predicted_emotion)\n",
        "\n",
        "# # Define the paths to the audio files you want to test\n",
        "# audio_files = [\n",
        "#     '/content/drive/MyDrive/Datasets/TESS Toronto emotional speech set data/YAF_sad/YAF_white_sad.wav',\n",
        "#     '/content/drive/MyDrive/Datasets/TESS Toronto emotional speech set data/YAF_pleasant_surprised/YAF_yearn_ps.wav',\n",
        "#     '/content/drive/MyDrive/Datasets/TESS Toronto emotional speech set data/YAF_neutral/YAF_white_neutral.wav',\n",
        "#     '/content/drive/MyDrive/Datasets/TESS Toronto emotional speech set data/YAF_happy/YAF_white_happy.wav',\n",
        "#     '/content/drive/MyDrive/Datasets/TESS Toronto emotional speech set data/YAF_fear/YAF_wag_fear.wav',\n",
        "#     '/content/drive/MyDrive/Datasets/TESS Toronto emotional speech set data/YAF_disgust/YAF_wire_disgust.wav',\n",
        "#     '/content/drive/MyDrive/Datasets/TESS Toronto emotional speech set data/YAF_angry/YAF_wife_angry.wav',\n",
        "#     '/content/drive/MyDrive/Datasets/TESS Toronto emotional speech set data/OAF_Sad/OAF_whip_sad.wav',\n",
        "#     '/content/drive/MyDrive/Datasets/TESS Toronto emotional speech set data/OAF_Pleasant_surprise/OAF_week_ps.wav',\n",
        "#     '/content/drive/MyDrive/Datasets/TESS Toronto emotional speech set data/OAF_neutral/OAF_when_neutral.wav',\n",
        "#     '/content/drive/MyDrive/Datasets/TESS Toronto emotional speech set data/OAF_happy/OAF_wife_happy.wav',\n",
        "#     '/content/drive/MyDrive/Datasets/TESS Toronto emotional speech set data/OAF_Fear/OAF_young_fear.wav',\n",
        "#     '/content/drive/MyDrive/Datasets/TESS Toronto emotional speech set data/OAF_disgust/OAF_wife_disgust.wav',\n",
        "#     '/content/drive/MyDrive/Datasets/TESS Toronto emotional speech set data/OAF_angry/OAF_wife_angry.wav',\n",
        "\n",
        "# ]\n",
        "\n",
        "# # Define the ground truth emotions for each audio file\n",
        "# ground_truth_emotions = [\n",
        "#     'sad',\n",
        "#     'ps',\n",
        "#     'neutral',\n",
        "#     'happy',\n",
        "#     'fear',\n",
        "#     'disgust',\n",
        "#     'angry',\n",
        "#     'sad',\n",
        "#     'ps',\n",
        "#     'natural',\n",
        "#     'happy',\n",
        "#     'fear',\n",
        "#     'disgust',\n",
        "#     'angry'\n",
        "\n",
        "# ]\n",
        "\n",
        "# # Calculate the validation accuracy of predicted emotions\n",
        "# total_samples = len(audio_files)\n",
        "# correct_predictions = 0\n",
        "\n",
        "# for audio_file, true_emotion in zip(audio_files, ground_truth_emotions):\n",
        "#     predicted_emotion = predict_emotion(audio_file)\n",
        "#     print(f\"Predicted Emotion: {predicted_emotion}\")\n",
        "#     if predicted_emotion == true_emotion:\n",
        "#         correct_predictions += 1\n",
        "# print(f\"Correct Predictions: {correct_predictions}\")\n",
        "# validation_accuracy = (correct_predictions / total_samples) * 100\n",
        "# print(f\"Validation Accuracy: {validation_accuracy}%\")\n"
      ],
      "metadata": {
        "id": "P7JFxiIoQeG-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}